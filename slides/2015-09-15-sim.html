---
layout: slide
title: CS 5220
description: Parallelism and locality in simulation
theme: black
transition: slide
js: js/quiz.js
---

<section data-markdown>
# CS 5220
## Parallelism and locality in simulation
## 15 Sep 2015
</section>


<section data-markdown>
### Parallelism and locality

-   Real world has *parallelism* and *locality*
    -   Particles, people, etc function independently
    -   Near interactions stronger than distant ones
    -   Can often simplify dependence on distant objects
-   More parallelism / locality through model
    -   Limited dependency between adjacent time steps
    -   Can neglect or approximate far-field effects
-   Often get parallelism at multiple levels
    -   Hierarchical circuit simulation
    -   Interacting models for climate
    -   Parallelizing trials/fevals in MC or optimization
</section>


<section>
<h3>Basic styles of simulation</h3>

<ul>
<li>Discrete event systems (continuous or discrete time)</li>
<li>Particle systems</li>
<li>Lumped parameter models (ODEs)</li>
<li>Distributed parameter models (PDEs / integral equations)</li>
</ul>

<p style="text-align:left;">
Often more than one type of simulation appropriate.<br/>
Sometimes more than one at a time!
</p>
</section>


<section data-markdown>
### Basic styles of simulation
####  Discrete event systems

TODO: Graphic

- Continuous or discrete time
- Game of life, logic-level circuit simulation
- Network simulation
</section>


<section data-markdown>
### Basic styles of simulation
#### Particle systems

TODO: Graphic

-   Billiards, electrons, galaxies, ...
-   Ants, cars, ...?
</section>


<section data-markdown>
### Basic styles of simulation
#### Lumped parameter models

TODO: Graphic

- Systems of Ordinary Differential Equations (ODEs)
- Circuits (SPICE), structures, chemical kinetics
</section>


<section data-markdown>
### Basic styles of simulation
#### Distributed parameter models

TODO: Graphic

- PDEs / integral equations
- Heat, elasticity, electrostatics, ...
</section>


<section data-markdown>
### Discrete events

Basic setup:

-   Finite set of variables, updated via transition function
-   *Synchronous* case: finite state machine
-   *Asynchronous* case: event-driven simulation
-   Synchronous example: Game of Life

Nice starting point — no discretization concerns!
</section>

<section>
<h3>Game of Life</h3>

<img style="background-color: white;" width="60%" src="/img/life/life.svg"></img>

<p style="text-align:left;">
Game of Life (John Conway):
</p>
<ol>
<li>Live cell dies with &lt; 2 live neighbors</li>
<li>Live cell dies with &gt; 3 live neighbors</li>
<li>Live cell lives with 2–3 live neighbors</li>
<li>Dead cell becomes live with exactly 3 live neighbors</li>
</ol>
</section>

<section data-markdown>
### Game of Life

TODO: Graphic

Easy to parallelize by *domain decomposition*.

-   Update work involves *volume* of subdomains
-   Communication per step on *surface* (cyan)
</section>

<section data-markdown>
### Game of Life: Pioneers and Settlers

What if pattern is “dilute”?

-   Few or no live cells at surface at each step
-   Think of live cell at a surface as an “event”
-   Only communicate events!
    -   This is *asynchronous*
    -   Harder with message passing — when do you receive?
</section>

<section data-markdown>
### Asynchronous Game of Life

How do we manage events?

-   Could be *speculative* — assume no communication across
    boundary for many steps, back up if needed
-   Or *conservative* — wait whenever communication
    possible
    -   possible $\not \equiv$ guaranteed!
    -   Deadlock: everyone waits for everyone else to send
    -   Can get around this with NULL messages

How do we manage load balance?

-   No need to simulate quiescent parts of the game!
-   Maybe dynamically assign smaller blocks to processors?
</section>

<section data-markdown>
### Particle simulation

Particles move via Newton ($F = ma$), with

-   External forces: ambient gravity, currents, etc.
-   Local forces: collisions, Van der Waals ($1/r^6$), etc.
-   Far-field forces: gravity and electrostatics ($1/r^2$), etc.
    -   Simple approximations often apply (Saint-Venant)
</section>

<section>
<h3>A forced example</h3>

<p style="text-align:left;">
Example force:
$$f_i = \sum_j Gm_i m_j
      \frac{(x_j-x_i)}{r_{ij}^3}
      \left(1 - \left(\frac{a}{r_{ij}}\right)^{4} \right), \qquad
      r_{ij} = \|x_i-x_j\|$$
</p>

<ul>
  <li>Long-range attractive force ($r^{-2}$)</li>
  <li>Short-range repulsive force ($r^{-6}$)</li>
  <li>Go from attraction to repulsion at radius $a$</li>
</ul>
</section>

<section data-markdown>
### A simple serial simulation

In Matlab, we can write

      npts = 100;
      t = linspace(0, tfinal, npts);
      [tout, xyv] = ode113(@fnbody, ...
                    t, [x; v], [], m, g);
      xout = xyv(:,1:length(x))';

... but I can’t call ode113 in C in parallel (or can I?)
</section>

<section data-markdown>
### A simple serial simulation

Maybe a fixed step leapfrog will do?

      npts = 100;
      steps_per_pt = 10;
      dt = tfinal/(steps_per_pt*(npts-1));
      xout = zeros(2*n, npts);
      xout(:,1) = x;
      for i = 1:npts-1
        for ii = 1:steps_per_pt
          x = x + v*dt;
          a = fnbody(x, m, g);
          v = v + a*dt;
        end
        xout(:,i+1) = x;
      end

</section>

<section data-markdown>
### Plotting particles

</section>

<section data-markdown>
### Pondering particles

-   Where do particles “live” (esp. in distributed memory)?
    -   Decompose in space? By particle number?
    -   What about clumping?
-   How are long-range force computations organized?
-   How are short-range force computations organized?
-   How is force computation load balanced?
-   What are the boundary conditions?
-   How are potential singularities handled?
-   What integrator is used? What step control?
</section>

<section data-markdown>
### External forces

Simplest case: no particle interactions.

-   Embarrassingly parallel (like Monte Carlo)!
-   Could just split particles evenly across processors
-   Is it that easy?
    -   Maybe some trajectories need short time steps?
    -   Even with MC, load balance may not be entirely trivial.
</section>

<section data-markdown>
### Local forces

TODO: Graphic

-   Simplest all-pairs check is $O(n^2)$ (expensive)
-   Or only check close pairs (via binning, quadtrees?)
-   Communication required for pairs checked
-   Usual model: domain decomposition
</section>

<section data-markdown>
### Local forces: Communication

TODO: Graphic

Minimize communication:

-   Send particles that might affect a neighbor “soon”
-   Trade extra computation against communication
-   Want low surface area-to-volume ratios on domains
</section>

<section data-markdown>
### Local forces: Load balance

TODO: Graphic

-   Are particles evenly distributed?
-   Do particles remain evenly distributed?
-   Can divide space unevenly (e.g. quadtree/octtree)
</section>

<section data-markdown>
### Far-field forces

TODO: Graphic

-   Every particle affects every other particle
-   All-to-all communication required
    -   Overlap communication with computation
    -   Poor memory scaling if everyone keeps everything!
-   Idea: pass particles in a round-robin manner
</section>

<section data-markdown>
### Passing particles for far-field forces

TODO: Graphic

    copy local particles to current buf
    for phase = 1:p
      send current buf to rank+1 (mod p)
      recv next buf from rank-1 (mod p) 
      interact local particles with current buf
      swap current buf with next buf
    end

</section>

<section>
<h3>Passing particles for far-field forces</h3>

<p style="text-align:left;">
Suppose $n = N/p$ particles in buffer. At each phase
$$
\begin{align}
  t_{\mathrm{comm}} &\approx \alpha + \beta n \\
  t_{\mathrm{comp}} &\approx \gamma n^2
\end{align}
$$
So we can mask communication with computation if
$$
  n \geq \frac{1}{2\gamma} \left( \beta + \sqrt{\beta^2 + 4 \alpha \gamma}
\right) > \frac{\beta}{\gamma}
$$
</p>

<p style="text-align:left;">
More efficient serial code<br/>
$\implies$ larger $n$ needed to mask communication!<br/>
$\implies$ worse speed-up as $p$ gets larger (fixed $N$)<br/>
but scaled speed-up ($n$ fixed) remains unchanged.
</p>

<p>
  This analysis neglects overhead term in LogP.
</p>
</section>

<section data-markdown>
### Far-field forces: particle-mesh methods

TODO: Graphic

Consider $r^{-2}$ electrostatic potential interaction

-   Enough charges looks like a continuum!
-   Poisson equation maps charge distribution to potential
-   Use fast Poisson solvers for regular grids (FFT, multigrid)
-   Approximation depends on mesh and particle density
-   Can clean up leading part of approximation error
</section>

<section data-markdown>
### Far-field forces: particle-mesh methods

TODO: Graphic

-   Map particles to mesh points (multiple strategies)
-   Solve potential PDE on mesh
-   Interpolate potential to particles
-   Add correction term – acts like local force

</section>

<section data-markdown>
### Far-field forces: tree methods

TODO: Graphic

-   Distance simplifies things
    -   Andromeda looks like a point mass from here?
-   Build a tree, approximating descendants at each node
-   Several variants: Barnes-Hut, FMM, Anderson’s method
-   More on this later in the semester

</section>

<section data-markdown>
### Summary of particle example

-   Model: Continuous motion of particles
    -   Could be electrons, cars, whatever...
-   Step through discretized time
-   Local interactions
    -   Relatively cheap
    -   Load balance a pain
-   All-pairs interactions
    -   Obvious algorithm is expensive ($O(n^2)$)
    -   Particle-mesh and tree-based algorithms help

An important special case of lumped/ODE models.
</section>
